import streamlit as st
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_core.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough, RunnableLambda
from langchain.schema import Document
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor
from sentence_transformers import CrossEncoder
import tempfile
import os
import dotenv
import PyPDF2
import io
import json
from datetime import datetime
import uuid

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
dotenv.load_dotenv()

# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ğŸš€ Advanced RAG ChatBot",
    page_icon="ğŸš€",
    layout="wide"
)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "projects" not in st.session_state:
    st.session_state.projects = {}
if "current_project" not in st.session_state:
    st.session_state.current_project = None
if "chat_history" not in st.session_state:
    st.session_state.chat_history = {}

# í”„ë¡œì íŠ¸ ê´€ë¦¬ í•¨ìˆ˜ë“¤
def create_new_project(name, description=""):
    """ìƒˆ í”„ë¡œì íŠ¸ ìƒì„±"""
    project_id = str(uuid.uuid4())[:8]
    project = {
        "id": project_id,
        "name": name,
        "description": description,
        "created_at": datetime.now().strftime("%Y-%m-%d %H:%M"),
        "documents": [],
        "vector_store": None,
        "total_chunks": 0,
        "processed_documents": []  # ì‹¤ì œ ì²˜ë¦¬ëœ ë¬¸ì„œ ë‚´ìš© ì €ì¥
    }
    st.session_state.projects[project_id] = project
    st.session_state.chat_history[project_id] = []
    return project_id

def get_current_project():
    """í˜„ì¬ ì„ íƒëœ í”„ë¡œì íŠ¸ ë°˜í™˜"""
    if st.session_state.current_project and st.session_state.current_project in st.session_state.projects:
        return st.session_state.projects[st.session_state.current_project]
    return None

# Reranker ì„¤ì • (ìºì‹œë¨)
@st.cache_resource
def load_reranker():
    """BGE reranker ëª¨ë¸ ë¡œë“œ"""
    try:
        return CrossEncoder("BAAI/bge-reranker-base")
    except:
        st.warning("âš ï¸ Reranker ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨. ê¸°ë³¸ ê²€ìƒ‰ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return None

# ì ìˆ˜ ê¸°ë°˜ rerank í•¨ìˆ˜
def rerank_docs(query, docs, model, top_n=5):
    """ë¬¸ì„œë“¤ì„ ì ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ì¬ìˆœìœ„ ì¡°ì •"""
    if model is None:
        return docs[:top_n]
    
    pairs = [[query, doc.page_content] for doc in docs]
    scores = model.predict(pairs)
    reranked = sorted(zip(scores, docs), key=lambda x: x[0], reverse=True)
    return [doc for score, doc in reranked[:top_n]]

def process_pdf_file(file_content, file_name):
    """PDF íŒŒì¼ ì²˜ë¦¬ - ê°œì„ ëœ ë²„ì „"""
    documents = []
    
    try:
        # PyPDF2ë¡œ ë¨¼ì € ì‹œë„
        pdf_reader = PyPDF2.PdfReader(io.BytesIO(file_content))
        st.info(f"ğŸ“„ {file_name}: {len(pdf_reader.pages)}í˜ì´ì§€ ê°ì§€ë¨")
        
        for page_num, page in enumerate(pdf_reader.pages):
            text = page.extract_text()
            if text.strip():  # ë¹ˆ í˜ì´ì§€ ì œì™¸
                doc = Document(
                    page_content=text,
                    metadata={
                        "page": page_num + 1, 
                        "source": file_name, 
                        "file_type": "pdf",
                        "total_pages": len(pdf_reader.pages)
                    }
                )
                documents.append(doc)
                
        st.success(f"âœ… {file_name}: {len(documents)}í˜ì´ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ")
        return documents
        
    except Exception as e:
        st.warning(f"âš ï¸ PyPDF2 ì‹¤íŒ¨, LangChain ë¡œë”ë¡œ ì¬ì‹œë„: {str(e)}")
        
        # ëŒ€ì²´ ë°©ë²•: LangChain PyPDFLoader
        try:
            with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
                tmp_file.write(file_content)
                tmp_path = tmp_file.name
            
            loader = PyPDFLoader(tmp_path)
            documents = loader.load()
            
            # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            for i, doc in enumerate(documents):
                doc.metadata.update({
                    "source": file_name,
                    "file_type": "pdf",
                    "total_pages": len(documents)
                })
            
            os.unlink(tmp_path)
            st.success(f"âœ… {file_name}: LangChainìœ¼ë¡œ {len(documents)}í˜ì´ì§€ ì²˜ë¦¬ ì™„ë£Œ")
            return documents
            
        except Exception as e2:
            st.error(f"âŒ {file_name} PDF ì²˜ë¦¬ ì‹¤íŒ¨: {str(e2)}")
            return []

def process_text_file(file_content, file_name):
    """í…ìŠ¤íŠ¸ íŒŒì¼ ì²˜ë¦¬ - ê°œì„ ëœ ë²„ì „"""
    try:
        # ì¸ì½”ë”© ìë™ ê°ì§€
        try:
            text = file_content.decode('utf-8')
            encoding_used = 'utf-8'
        except UnicodeDecodeError:
            try:
                text = file_content.decode('cp949')
                encoding_used = 'cp949'
            except UnicodeDecodeError:
                text = file_content.decode('latin-1')
                encoding_used = 'latin-1'
        
        st.info(f"ğŸ“ {file_name}: {encoding_used} ì¸ì½”ë”©ìœ¼ë¡œ ì½ê¸° ì™„ë£Œ")
        
        doc = Document(
            page_content=text,
            metadata={
                "source": file_name, 
                "file_type": "txt",
                "encoding": encoding_used,
                "char_count": len(text)
            }
        )
        
        st.success(f"âœ… {file_name}: {len(text):,}ì í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì™„ë£Œ")
        return [doc]
        
    except Exception as e:
        st.error(f"âŒ {file_name} í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
        return []

def process_multiple_documents(uploaded_files):
    """ì—¬ëŸ¬ ë¬¸ì„œ ì²˜ë¦¬ - ê°œì„ ëœ ë²„ì „"""
    all_documents = []
    processed_files = []
    
    st.info(f"ğŸ“š ì´ {len(uploaded_files)}ê°œ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘...")
    
    for i, uploaded_file in enumerate(uploaded_files):
        st.write(f"**{i+1}/{len(uploaded_files)}**: {uploaded_file.name} ì²˜ë¦¬ì¤‘...")
        
        file_content = uploaded_file.read()
        file_name = uploaded_file.name
        file_type = file_name.split('.')[-1].lower()
        
        try:
            if file_type == 'pdf':
                documents = process_pdf_file(file_content, file_name)
            elif file_type == 'txt':
                documents = process_text_file(file_content, file_name)
            else:
                st.warning(f"âš ï¸ {file_name}: ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤.")
                continue
            
            if documents:  # ë¬¸ì„œê°€ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ëœ ê²½ìš°ë§Œ
                all_documents.extend(documents)
                processed_files.append({
                    "name": file_name,
                    "type": file_type,
                    "size": len(file_content),
                    "pages": len(documents) if file_type == 'pdf' else 1,
                    "doc_count": len(documents)
                })
                
                # ê° ë¬¸ì„œì˜ ì²« 100ì ë¯¸ë¦¬ë³´ê¸°
                preview = documents[0].page_content[:100] + "..." if len(documents[0].page_content) > 100 else documents[0].page_content
                st.text_area(f"ğŸ“– {file_name} ë¯¸ë¦¬ë³´ê¸°", preview, height=100, key=f"preview_{i}")
            
        except Exception as e:
            st.error(f"âŒ {file_name} ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
    
    st.success(f"ğŸ‰ ì´ {len(all_documents)}ê°œ ë¬¸ì„œ ì²­í¬ ìƒì„± ì™„ë£Œ!")
    return all_documents, processed_files

def create_vector_store(documents, project_id, chunk_size=500, chunk_overlap=50):
    """ë²¡í„° ì €ì¥ì†Œ ìƒì„± - ì²­í¬ í¬ê¸° ì¡°ì • ê°€ëŠ¥"""
    if not documents:
        return None, 0
    
    st.info(f"ğŸ”„ ì²­í¬ í¬ê¸° {chunk_size}, ì˜¤ë²„ë© {chunk_overlap}ë¡œ ë¬¸ì„œ ë¶„í•  ì¤‘...")
    
    # í…ìŠ¤íŠ¸ ìŠ¤í”Œë¦¬í„°
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separators=["\n\n", "\n", ". ", ".", " ", ""]
    )
    
    # ë¬¸ì„œ ë¶„í• 
    split_docs = splitter.split_documents(documents)
    
    # ë¶„í•  ê²°ê³¼ ìƒì„¸ ì •ë³´
    st.info(f"ğŸ“Š ë¶„í•  ê²°ê³¼: {len(documents)}ê°œ ì›ë³¸ ë¬¸ì„œ â†’ {len(split_docs)}ê°œ ì²­í¬")
    
    # ê° ì›ë³¸ ë¬¸ì„œë³„ ì²­í¬ ìˆ˜ í‘œì‹œ
    source_chunks = {}
    for doc in split_docs:
        source = doc.metadata.get('source', 'Unknown')
        source_chunks[source] = source_chunks.get(source, 0) + 1
    
    for source, count in source_chunks.items():
        st.write(f"  - {source}: {count}ê°œ ì²­í¬")
    
    # ì„ë² ë”© ìƒì„±
    st.info("ğŸ”„ ì„ë² ë”© ìƒì„± ì¤‘...")
    embeddings = OpenAIEmbeddings()
    
    # ë²¡í„° ì €ì¥ì†Œ ìƒì„±
    vector_store = FAISS.from_documents(embedding=embeddings, documents=split_docs)
    
    st.success("âœ… ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì™„ë£Œ!")
    return vector_store, len(split_docs)

def format_docs(docs):
    """ë¬¸ì„œ í¬ë§·íŒ… - ì¶œì²˜ ì •ë³´ í¬í•¨"""
    formatted = []
    for doc in docs:
        source = doc.metadata.get('source', 'Unknown')
        page = doc.metadata.get('page', '')
        page_info = f" (í˜ì´ì§€ {page})" if page else ""
        
        formatted.append(f"[ì¶œì²˜: {source}{page_info}]\n{doc.page_content}")
    
    return "\n\n---\n\n".join(formatted)

# ë©”ì¸ UI
st.title("ğŸš€ Advanced RAG ChatBot")
st.markdown("**ì—¬ëŸ¬ ë¬¸ì„œ ì—…ë¡œë“œ + í”„ë¡œì íŠ¸ë³„ ê´€ë¦¬ + ìŠ¤ë§ˆíŠ¸ ì±„íŒ…**")

# ì‚¬ì´ë“œë°” - í”„ë¡œì íŠ¸ ê´€ë¦¬
with st.sidebar:
    st.header("ğŸ“ í”„ë¡œì íŠ¸ ê´€ë¦¬")
    
    # ìƒˆ í”„ë¡œì íŠ¸ ìƒì„±
    with st.expander("â• ìƒˆ í”„ë¡œì íŠ¸ ë§Œë“¤ê¸°"):
        new_project_name = st.text_input("í”„ë¡œì íŠ¸ ì´ë¦„", placeholder="ì˜ˆ: AI íŠ¸ë Œë“œ ë¶„ì„")
        new_project_desc = st.text_area("ì„¤ëª… (ì„ íƒ)", placeholder="ì´ í”„ë¡œì íŠ¸ì— ëŒ€í•œ ê°„ë‹¨í•œ ì„¤ëª…")
        
        if st.button("í”„ë¡œì íŠ¸ ìƒì„±", type="primary"):
            if new_project_name:
                project_id = create_new_project(new_project_name, new_project_desc)
                st.session_state.current_project = project_id
                st.success(f"âœ… '{new_project_name}' í”„ë¡œì íŠ¸ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")
                st.rerun()
            else:
                st.error("í”„ë¡œì íŠ¸ ì´ë¦„ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    
    # ê¸°ì¡´ í”„ë¡œì íŠ¸ ëª©ë¡
    st.subheader("ğŸ“‹ í”„ë¡œì íŠ¸ ëª©ë¡")
    
    if st.session_state.projects:
        for project_id, project in st.session_state.projects.items():
            is_current = project_id == st.session_state.current_project
            
            with st.container():
                col1, col2 = st.columns([3, 1])
                
                with col1:
                    if st.button(
                        f"{'ğŸŸ¢' if is_current else 'âšª'} {project['name']}", 
                        key=f"select_{project_id}",
                        help=f"ìƒì„±ì¼: {project['created_at']}\në¬¸ì„œ ìˆ˜: {len(project['documents'])}",
                        use_container_width=True
                    ):
                        st.session_state.current_project = project_id
                        st.rerun()
                
                with col2:
                    if st.button("ğŸ—‘ï¸", key=f"delete_{project_id}", help="í”„ë¡œì íŠ¸ ì‚­ì œ"):
                        del st.session_state.projects[project_id]
                        del st.session_state.chat_history[project_id]
                        if st.session_state.current_project == project_id:
                            st.session_state.current_project = None
                        st.rerun()
    else:
        st.info("í”„ë¡œì íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ìƒˆ í”„ë¡œì íŠ¸ë¥¼ ë§Œë“¤ì–´ë³´ì„¸ìš”!")
    
    # í˜„ì¬ í”„ë¡œì íŠ¸ ì •ë³´
    current_project = get_current_project()
    if current_project:
        st.markdown("---")
        st.subheader("ğŸ“Š í˜„ì¬ í”„ë¡œì íŠ¸")
        st.write(f"**ì´ë¦„:** {current_project['name']}")
        if current_project['description']:
            st.write(f"**ì„¤ëª…:** {current_project['description']}")
        st.write(f"**ë¬¸ì„œ ìˆ˜:** {len(current_project['documents'])}")
        st.write(f"**ì²­í¬ ìˆ˜:** {current_project['total_chunks']}")
        st.write(f"**ìƒì„±ì¼:** {current_project['created_at']}")

# ë©”ì¸ ì˜ì—­
if not current_project:
    st.info("ğŸ‘ˆ ë¨¼ì € í”„ë¡œì íŠ¸ë¥¼ ì„ íƒí•˜ê±°ë‚˜ ìƒˆë¡œ ë§Œë“¤ì–´ì£¼ì„¸ìš”!")
else:
    # íƒ­ìœ¼ë¡œ êµ¬ì„±
    tab1, tab2, tab3 = st.tabs(["ğŸ“¤ ë¬¸ì„œ ì—…ë¡œë“œ", "ğŸ’¬ ì±„íŒ…", "ğŸ“Š í”„ë¡œì íŠ¸ ìƒì„¸"])
    
    with tab1:
        st.subheader("ğŸ“¤ ë¬¸ì„œ ì—…ë¡œë“œ")
        
        col1, col2 = st.columns([2, 1])
        
        with col1:
            uploaded_files = st.file_uploader(
                "ë¬¸ì„œë¥¼ ì„ íƒí•˜ì„¸ìš” (PDF, TXT)",
                type=['pdf', 'txt'],
                accept_multiple_files=True,
                help="ì—¬ëŸ¬ íŒŒì¼ì„ ë™ì‹œì— ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            )
        
        with col2:
            # LLM ì„¤ì •
            st.subheader("âš™ï¸ AI ì„¤ì •")
            model_name = st.selectbox(
                "ëª¨ë¸ ì„ íƒ",
                ["gpt-4o-mini", "gpt-4o", "gpt-3.5-turbo"],
                index=0
            )
            temperature = st.slider("ì°½ì˜ì„±", 0.0, 1.0, 0.1, 0.1)
            
            # ì²­í¬ í¬ê¸° ì„¤ì •
            st.subheader("ğŸ“ ì²­í¬ ì„¤ì •")
            chunk_size = st.slider(
                "ì²­í¬ í¬ê¸°", 
                min_value=100, 
                max_value=2000, 
                value=500, 
                step=50,
                help="ì²­í¬ í¬ê¸°ê°€ ì‘ì„ìˆ˜ë¡ ë” ì •í™•í•œ ê²€ìƒ‰ì´ ê°€ëŠ¥í•˜ì§€ë§Œ ë¬¸ë§¥ì´ ë‹¨ì ˆë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í° ì²­í¬ëŠ” ë” ë§ì€ ë¬¸ë§¥ì„ í¬í•¨í•˜ì§€ë§Œ ë…¸ì´ì¦ˆê°€ ì¦ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            )
            
            chunk_overlap = st.slider(
                "ì²­í¬ ì˜¤ë²„ë©", 
                min_value=0, 
                max_value=200, 
                value=50, 
                step=10,
                help="ì²­í¬ ê°„ ê²¹ì¹˜ëŠ” ë¶€ë¶„ì…ë‹ˆë‹¤. ë†’ì„ìˆ˜ë¡ ë¬¸ë§¥ ì—°ê²°ì´ ì¢‹ì•„ì§€ì§€ë§Œ ì¤‘ë³µ ì •ë³´ê°€ ì¦ê°€í•©ë‹ˆë‹¤."
            )
            
            # ì¶”ê°€ ì„¤ì •
            st.subheader("ğŸ”§ ê³ ê¸‰ ì„¤ì •")
            use_reranker = st.checkbox("ğŸ”„ Reranker ì‚¬ìš©", value=False, help="ê²€ìƒ‰ ê²°ê³¼ ì¬ìˆœìœ„ ì¡°ì •ìœ¼ë¡œ ì •í™•ë„ í–¥ìƒ")
            use_compression = st.checkbox("ğŸ—œï¸ ë¬¸ì„œ ì••ì¶•", value=False, help="LLM ê¸°ë°˜ ë¬¸ì„œ ì••ì¶•ìœ¼ë¡œ í•µì‹¬ ë‚´ìš©ë§Œ ì¶”ì¶œ")
        
        # ì²­í¬ í¬ê¸° ê°€ì´ë“œ
        st.info(f"""
        **ğŸ“ í˜„ì¬ ì²­í¬ ì„¤ì •**
        - **ì²­í¬ í¬ê¸°**: {chunk_size}ì 
        - **ì˜¤ë²„ë©**: {chunk_overlap}ì
        
        **ğŸ’¡ ì²­í¬ í¬ê¸° ê°€ì´ë“œ**
        - **100-300ì**: ë§¤ìš° ì •í™•í•œ ê²€ìƒ‰, ì§§ì€ ë‹µë³€ì— ì í•©
        - **300-500ì**: ê· í˜•ì¡íŒ ì„¤ì •, ëŒ€ë¶€ë¶„ì˜ ìš©ë„ì— ì í•© 
        - **500-1000ì**: ê¸´ ë¬¸ë§¥ ìœ ì§€, ë³µì¡í•œ ì§ˆë¬¸ì— ì í•©
        - **1000ì+**: ë§¤ìš° ê¸´ ë¬¸ë§¥, ìš”ì•½ì´ë‚˜ ì „ì²´ì ì¸ ì´í•´ì— ì í•©
        """)
        
        if uploaded_files:
            if st.button("ğŸ“š ë¬¸ì„œ ì²˜ë¦¬í•˜ê¸°", type="primary"):
                with st.spinner("ğŸ“– ë¬¸ì„œë“¤ì„ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                    try:
                        documents, processed_files = process_multiple_documents(uploaded_files)
                        
                        if documents:
                            vector_store, total_chunks = create_vector_store(
                                documents, 
                                current_project['id'],
                                chunk_size=chunk_size,
                                chunk_overlap=chunk_overlap
                            )
                            
                            # í”„ë¡œì íŠ¸ ì—…ë°ì´íŠ¸
                            current_project['documents'].extend(processed_files)
                            current_project['vector_store'] = vector_store
                            current_project['total_chunks'] = total_chunks
                            current_project['processed_documents'] = documents  # ì›ë³¸ ë¬¸ì„œ ì €ì¥
                            current_project['chunk_settings'] = {
                                'chunk_size': chunk_size,
                                'chunk_overlap': chunk_overlap
                            }
                            
                            st.success(f"âœ… {len(processed_files)}ê°œ ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ!")
                            
                            # ì²˜ë¦¬ëœ íŒŒì¼ ì •ë³´ í‘œì‹œ
                            for file_info in processed_files:
                                st.write(f"- **{file_info['name']}** ({file_info['type'].upper()}) - {file_info['size']:,} bytes - {file_info['doc_count']}ê°œ ì²­í¬")
                        else:
                            st.error("ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
                            
                    except Exception as e:
                        st.error(f"âŒ ë¬¸ì„œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                        st.exception(e)  # ë””ë²„ê¹…ìš©
    
    with tab2:
        st.subheader("ğŸ’¬ AIì™€ ì±„íŒ…í•˜ê¸°")
        
        if current_project['vector_store'] is None:
            st.warning("ğŸ“¤ ë¨¼ì € ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  ì²˜ë¦¬í•´ì£¼ì„¸ìš”!")
        else:
            # LLM ì´ˆê¸°í™”
            llm = ChatOpenAI(model=model_name, temperature=temperature)
            
            # ê²€ìƒ‰ ì„¤ì • í‘œì‹œ
            if 'chunk_settings' in current_project:
                settings = current_project['chunk_settings']
                st.info(f"ğŸ”§ í˜„ì¬ ì„¤ì •: ì²­í¬ í¬ê¸° {settings['chunk_size']}ì, ì˜¤ë²„ë© {settings['chunk_overlap']}ì")
            
            # ê¸°ë³¸ ë¦¬íŠ¸ë¦¬ë²„ (ë” ë§ì€ ë¬¸ì„œ ê²€ìƒ‰)
            base_retriever = current_project['vector_store'].as_retriever(
                search_type="similarity", 
                search_kwargs={"k": 15}  # ë” ë§ì€ ë¬¸ì„œ ê²€ìƒ‰
            )
            
            # ê³ ê¸‰ ê²€ìƒ‰ ì„¤ì •
            retriever = base_retriever
            
            # 1. ë¬¸ì„œ ì••ì¶• ì˜µì…˜
            if use_compression:
                compressor = LLMChainExtractor.from_llm(llm)
                retriever = ContextualCompressionRetriever(
                    base_retriever=base_retriever,
                    base_compressor=compressor
                )
                st.info("ğŸ—œï¸ LLM ê¸°ë°˜ ë¬¸ì„œ ì••ì¶•ì´ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            # 2. Reranker ì˜µì…˜
            reranker_model = None
            if use_reranker:
                with st.spinner("ğŸ”„ Reranker ëª¨ë¸ì„ ë¡œë“œí•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                    reranker_model = load_reranker()
                if reranker_model:
                    st.info("ğŸ”„ BGE Rerankerê°€ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
            prompt = ChatPromptTemplate([
                ("system", 
                 "ë¬¸ì„œ: {context}\n\n"
                 "ë‹¹ì‹ ì€ ì—…ë¡œë“œëœ ë¬¸ì„œë“¤ì˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. "
                 "í•­ìƒ ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.\n"
                 "ë¬¸ì„œì—ì„œ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš° 'ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.\n"
                 "ë‹µë³€í•  ë•ŒëŠ” ì–´ëŠ ë¬¸ì„œì—ì„œ ê°€ì ¸ì˜¨ ì •ë³´ì¸ì§€ë„ ì–¸ê¸‰í•´ì£¼ì„¸ìš”.\n"
                 "ì—¬ëŸ¬ ë¬¸ì„œì—ì„œ ì •ë³´ë¥¼ ì°¾ì€ ê²½ìš° ëª¨ë“  ì¶œì²˜ë¥¼ ëª…ì‹œí•˜ì„¸ìš”."), 
                ("user", "{query}")
            ])
            
            # RAG ì²´ì¸ ìƒì„± (Reranker í¬í•¨)
            def enhanced_retrieval(query):
                docs = retriever.invoke(query)
                if reranker_model and docs:
                    docs = rerank_docs(query, docs, reranker_model, top_n=10)
                return format_docs(docs)
            
            chain = {
                "context": RunnableLambda(enhanced_retrieval),
                "query": RunnablePassthrough()
            } | prompt | llm
            
            # ì±„íŒ… ê¸°ë¡ í‘œì‹œ
            chat_history = st.session_state.chat_history[current_project['id']]
            
            for message in chat_history:
                with st.chat_message(message["role"]):
                    st.markdown(message["content"])
            
            # ìƒˆë¡œìš´ ì§ˆë¬¸ ì…ë ¥
            if query := st.chat_input("ë¬¸ì„œì— ëŒ€í•´ ì§ˆë¬¸í•´ë³´ì„¸ìš”!"):
                # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
                chat_history.append({"role": "user", "content": query})
                with st.chat_message("user"):
                    st.markdown(query)
                
                # AI ì‘ë‹µ ìƒì„±
                with st.chat_message("assistant"):
                    with st.spinner("ğŸ¤” ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                        try:
                            # ë””ë²„ê¹…: ê²€ìƒ‰ëœ ë¬¸ì„œ í‘œì‹œ
                            retrieved_docs = current_project['vector_store'].similarity_search(query, k=5)
                            
                            with st.expander("ğŸ” ê²€ìƒ‰ëœ ë¬¸ì„œ (ë””ë²„ê¹…)"):
                                for i, doc in enumerate(retrieved_docs):
                                    st.write(f"**ë¬¸ì„œ {i+1}**: {doc.metadata.get('source', 'Unknown')}")
                                    st.write(f"**ë‚´ìš©**: {doc.page_content[:200]}...")
                                    st.write("---")
                            
                            response = chain.invoke(query)
                            answer = response.content
                            st.markdown(answer)
                            
                            # ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ ì¶”ê°€
                            chat_history.append({"role": "assistant", "content": answer})
                            
                        except Exception as e:
                            error_msg = f"ì£„ì†¡í•©ë‹ˆë‹¤. ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                            st.error(error_msg)
                            st.exception(e)  # ë””ë²„ê¹…ìš©
                            chat_history.append({"role": "assistant", "content": error_msg})
            
            # ì±„íŒ… ì´ˆê¸°í™” ë²„íŠ¼
            if st.button("ğŸ—‘ï¸ ì±„íŒ… ê¸°ë¡ ì‚­ì œ"):
                st.session_state.chat_history[current_project['id']] = []
                st.rerun()
    
    with tab3:
        st.subheader("ğŸ“Š í”„ë¡œì íŠ¸ ìƒì„¸ ì •ë³´")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.metric("ì´ ë¬¸ì„œ ìˆ˜", len(current_project['documents']))
            st.metric("ì´ ì²­í¬ ìˆ˜", current_project['total_chunks'])
        
        with col2:
            total_chat = len(st.session_state.chat_history[current_project['id']])
            st.metric("ì±„íŒ… ë©”ì‹œì§€ ìˆ˜", total_chat)
            st.metric("í”„ë¡œì íŠ¸ ID", current_project['id'])
        
        # ì²­í¬ ì„¤ì • ì •ë³´
        if 'chunk_settings' in current_project:
            settings = current_project['chunk_settings']
            st.subheader("ğŸ“ ì²­í¬ ì„¤ì •")
            col1, col2 = st.columns(2)
            with col1:
                st.metric("ì²­í¬ í¬ê¸°", f"{settings['chunk_size']}ì")
            with col2:
                st.metric("ì²­í¬ ì˜¤ë²„ë©", f"{settings['chunk_overlap']}ì")
        
        # ì—…ë¡œë“œëœ ë¬¸ì„œ ëª©ë¡
        if current_project['documents']:
            st.subheader("ğŸ“„ ì—…ë¡œë“œëœ ë¬¸ì„œë“¤")
            for doc in current_project['documents']:
                with st.expander(f"ğŸ“ {doc['name']}"):
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.write(f"**íƒ€ì…:** {doc['type'].upper()}")
                    with col2:
                        st.write(f"**í¬ê¸°:** {doc['size']:,} bytes")
                    with col3:
                        st.write(f"**ì²­í¬ ìˆ˜:** {doc['doc_count']}")
                        if doc['type'] == 'pdf':
                            st.write(f"**í˜ì´ì§€:** {doc['pages']}")
        
        # ë²¡í„° ì €ì¥ì†Œ í…ŒìŠ¤íŠ¸
        if current_project['vector_store'] is not None:
            st.subheader("ğŸ” ë²¡í„° ì €ì¥ì†Œ í…ŒìŠ¤íŠ¸")
            test_query = st.text_input("í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”:")
            if test_query:
                with st.spinner("ê²€ìƒ‰ ì¤‘..."):
                    results = current_project['vector_store'].similarity_search(test_query, k=3)
                    st.write(f"**ê²€ìƒ‰ ê²°ê³¼ ({len(results)}ê°œ):**")
                    for i, doc in enumerate(results):
                        st.write(f"**{i+1}. {doc.metadata.get('source', 'Unknown')}**")
                        st.write(f"ë‚´ìš©: {doc.page_content[:300]}...")
                        st.write("---")

# í•˜ë‹¨ ì •ë³´
st.markdown("---")
st.markdown("ğŸš€ **Advanced RAG ChatBot** - LangChain + Streamlit + FAISS")